{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-18T17:56:50.174282Z",
     "start_time": "2024-11-18T17:56:50.169429Z"
    }
   },
   "source": [
    "import string\n",
    "from pyexpat import model\n",
    "\n",
    "from lightgbm import train\n",
    "from unidecode import unidecode\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ],
   "outputs": [],
   "execution_count": 164
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T17:56:50.191294Z",
     "start_time": "2024-11-18T17:56:50.185819Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Word2Vec(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, batch_size):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        \n",
    "        self.cbow = nn.Linear(vocab_size, batch_size)\n",
    "        self.skip_gram = nn.Linear(batch_size, vocab_size, bias=False)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        latent_space = self.cbow(input)\n",
    "        return self.skip_gram(latent_space)"
   ],
   "id": "e53cbf3023fee2e8",
   "outputs": [],
   "execution_count": 165
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T17:56:50.320625Z",
     "start_time": "2024-11-18T17:56:50.214568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def preprocess_text(text):\n",
    "    illegal_chars = string.punctuation + \"-\" + \"\\n\"\n",
    "    \n",
    "    for ch in illegal_chars:\n",
    "        text = text.replace(ch, '')\n",
    "        \n",
    "    \n",
    "    return text.lower()\n",
    "\n",
    "text_lines = open(\"training_text\", 'r').readlines()\n",
    "text = \"\"\n",
    "\n",
    "for line in text_lines:\n",
    "    text = text + preprocess_text(line);\n",
    "\n",
    "text = text.split(' ')\n",
    "\n",
    "for i in range(len(text)):\n",
    "    text[i] = unidecode(text[i])\n",
    "    \n",
    "print(len(text))\n"
   ],
   "id": "99c44555bf82bed7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36959\n"
     ]
    }
   ],
   "execution_count": 166
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T17:56:50.368359Z",
     "start_time": "2024-11-18T17:56:50.358655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## One Hot Encoding\n",
    "\n",
    "map_vf = {}\n",
    "\n",
    "word_order = [];\n",
    "\n",
    "it = 0;\n",
    "\n",
    "for word in text:\n",
    "    if word not in map_vf:\n",
    "        map_vf[word] = it\n",
    "        it += 1"
   ],
   "id": "559921bb1ca80ace",
   "outputs": [],
   "execution_count": 167
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T17:56:51.789883Z",
     "start_time": "2024-11-18T17:56:50.373046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Adding all One Hot Encoded data to training set\n",
    "\n",
    "training_data = []\n",
    "\n",
    "one_hot = np.zeros(it + 1)\n",
    "\n",
    "context_size = 3\n",
    "\n",
    "for i in range(context_size, len(text) - context_size, 1):\n",
    "    X = one_hot.copy()\n",
    "    for offset in range(-context_size + 1, context_size, 1):\n",
    "        X[map_vf[text[i - offset]]] = 1\n",
    "    training_data.append(X)"
   ],
   "id": "81f4efab0d42aa51",
   "outputs": [],
   "execution_count": 168
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T17:59:01.781777Z",
     "start_time": "2024-11-18T17:56:51.811716Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_size = 5000\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "model = Word2Vec(len(map_vf) + 1, batch_size).to(device)\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "def train(epoch):\n",
    "    print(\"Epoch: \", epoch)\n",
    "    \n",
    "    loss_total = 0;\n",
    "    \n",
    "    lenn = len(training_data) - (len(training_data) % batch_size)\n",
    "    \n",
    "    for i in range(0, lenn, batch_size):\n",
    "        X_train = torch.Tensor(training_data[i:i+batch_size]).reshape(batch_size, -1).to(device)\n",
    "        # print(X_train.shape)\n",
    "        \n",
    "        pred = model(X_train)\n",
    "        loss_item = loss(X_train, pred)\n",
    "        loss_total = loss_total + loss_item.item()\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        loss_item.backward()\n",
    "        opt.step()\n",
    "        \n",
    "    print(\"Avg loss: \", loss_total / len(training_data) * batch_size)\n",
    "    print(\"Total loss: \", loss_total)\n",
    "        \n",
    "for i in range(10):\n",
    "    train(i)\n"
   ],
   "id": "23a0b4a4b808585f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Avg loss:  0.0007541246672362017\n",
      "Total loss:  0.0055734337656758726\n",
      "Epoch:  1\n",
      "Avg loss:  0.0004725805963936605\n",
      "Total loss:  0.003492654155706987\n",
      "Epoch:  2\n",
      "Avg loss:  0.0003195118618164244\n",
      "Total loss:  0.0023613843659404665\n",
      "Epoch:  3\n",
      "Avg loss:  0.00026208530162262174\n",
      "Total loss:  0.0019369676301721483\n",
      "Epoch:  4\n",
      "Avg loss:  0.00021821545537651304\n",
      "Total loss:  0.0016127431445056573\n",
      "Epoch:  5\n",
      "Avg loss:  0.0001858723716615477\n",
      "Total loss:  0.0013737083500018343\n",
      "Epoch:  6\n",
      "Avg loss:  0.00016501370173841742\n",
      "Total loss:  0.0012195502640679479\n",
      "Epoch:  7\n",
      "Avg loss:  0.00014835251488878388\n",
      "Total loss:  0.0010964140965370461\n",
      "Epoch:  8\n",
      "Avg loss:  0.00013583539672730414\n",
      "Total loss:  0.001003905083052814\n",
      "Epoch:  9\n",
      "Avg loss:  0.00012537821355138127\n",
      "Total loss:  0.0009266202250728384\n"
     ]
    }
   ],
   "execution_count": 169
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T17:59:01.910603Z",
     "start_time": "2024-11-18T17:59:01.904935Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_word_embedding(model, word):\n",
    "    word = word.lower()\n",
    "    embeddings = model.skip_gram.weight.detach().cpu()\n",
    "    id = map_vf[word]\n",
    "    return embeddings[id]"
   ],
   "id": "702b9e56fc1b8d84",
   "outputs": [],
   "execution_count": 170
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T18:00:10.723905Z",
     "start_time": "2024-11-18T18:00:10.677594Z"
    }
   },
   "cell_type": "code",
   "source": "print(get_word_embedding(model, \"samadaul\"))",
   "id": "e6c7206aa0456ad4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.0115,  0.0045,  0.0211,  ...,  0.0064, -0.0295,  0.0047])\n"
     ]
    }
   ],
   "execution_count": 178
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
